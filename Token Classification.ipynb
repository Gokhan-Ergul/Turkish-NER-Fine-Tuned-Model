{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f2fb74-cdaf-4fe1-92a8-ef14765442b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6be300f8-1311-4843-8f11-0eb52aa963a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"erayyildiz/turkish_ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0ac6526-bd90-4b56-8fd6-8bcf663f09a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'domain', 'ner_tags'],\n",
       "        num_rows: 532629\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6132f688-e60a-4665-bf22-9a6a56f66f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a991a1b9-eb00-4750-b345-5854dfcd05fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['architecture', 'basketball', 'book', 'business', 'education', 'fictional_universe', 'film', 'food', 'geography', 'government', 'law', 'location', 'military', 'music', 'opera', 'organization', 'people', 'religion', 'royalty', 'soccer', 'sports', 'theater', 'time', 'travel', 'tv'], id=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.features['domain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a5a8dfd-ef1d-4366-8cb0-baaa62c781ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PERSON', 'I-PERSON', 'B-ORGANIZATION', 'I-ORGANIZATION', 'B-LOCATION', 'I-LOCATION', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.features['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "393a8e02-c1c5-4812-bb53-bfe7edcafca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Domain: represents the type of the senteence or where it comes from.\n",
    "#ner_tags: tokens' type and it is head or que."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69a537ad-13e3-4eb5-b0d1-1a2ae9fcbfae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['Corina',\n",
       "  'Casanova',\n",
       "  ',',\n",
       "  'İsviçre',\n",
       "  'Federal',\n",
       "  'Şansölyesidir',\n",
       "  '.\\n'],\n",
       " 'domain': 9,\n",
       " 'ner_tags': [1, 2, 0, 5, 0, 7, 0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "726a8f8f-7394-453c-b84d-fc1eb180016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you see the sentence already separated by words\n",
    "#which means this data already pre-tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e337a13b-cafe-4c81-9b86-aeab0fedc467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97940ceb-34fb-4221-b1d4-d2d303a0800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"dbmdz/bert-base-turkish-w\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82472061-14c1-4e6b-9b75-21dd8261e569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82abba56-ccc7-4a3c-a4c8-9a97880637e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 11317, 3930, 10628, 12398, 5477, 16, 11046, 17356, 16673, 16938, 2462, 2067, 18, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(train[0]['tokens'], is_split_into_words=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a33e268-df97-4c3e-9d84-5d670fba5f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Corina', 'Casanova', ',', 'İsviçre', 'Federal', 'Şansölyesidir', '.\\n']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be644e30-6604-4e72-b253-c54dc2aefc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Cor',\n",
       " '##ina',\n",
       " 'Cas',\n",
       " '##ano',\n",
       " '##va',\n",
       " ',',\n",
       " 'İsviçre',\n",
       " 'Federal',\n",
       " 'Şans',\n",
       " '##ölye',\n",
       " '##si',\n",
       " '##dir',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9a585d-df0d-4f37-aeac-479aa23729a2",
   "metadata": {},
   "source": [
    "## Label Aligment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "febdf3f6-fd6e-45c4-b825-9b94aa26e988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 0, 5, 0, 7, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d51ce88-f891-4428-8da2-3e861d38150f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 0, 1, 1, 1, 2, 3, 4, 5, 5, 5, 5, 6, None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a6f09c7-92d3-49ae-8f76-032b3acec2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6672fd16-57b3-447c-a716-4c3f8acd25ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 0, 5, 0, 7, 0]\n",
      "[-100, 1, 2, 2, 2, 2, 0, 5, 0, 7, 8, 8, 8, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = train[0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9db2a419-0a65-4429-95d5-7aead79d9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_align_labels(example):\n",
    "    tokenize  = tokenizer(example['tokens'], is_split_into_words=True)\n",
    "    all_labels = example['ner_tags']\n",
    "    new_labels = []\n",
    "    deleted_row_count = 0\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_inds = tokenize.word_ids(i)\n",
    "        \n",
    "        valid_word_inds = [w for w in word_inds if w is not None]\n",
    "        if len(valid_word_inds) > 0 and max(valid_word_inds) >= len(labels):\n",
    "            print(\"WARNING: word_inds out of range!\", word_inds, \"labels len:\", len(labels))\n",
    "            new_labels.append([-100]*len(word_ids))\n",
    "            deleted_row_count+=1\n",
    "            continue\n",
    "            \n",
    "        new_labels.append(align_labels_with_tokens(labels,word_inds))\n",
    "\n",
    "\n",
    "    tokenize['labels'] = new_labels\n",
    "    return tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e67bf4b-e5da-4e9b-80d7-fc86e86868c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_with_align_labels, batched = True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2751f56-cd27-4369-a698-9e1afc9389ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 532629\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad67a37e-7cb8-42c3-959f-f48b1dde53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8989681-52a6-41fa-a0a8-1f4b4aaac72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2,\n",
       "  11317,\n",
       "  3930,\n",
       "  10628,\n",
       "  12398,\n",
       "  5477,\n",
       "  16,\n",
       "  11046,\n",
       "  17356,\n",
       "  16673,\n",
       "  16938,\n",
       "  2462,\n",
       "  2067,\n",
       "  18,\n",
       "  3],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [-100, 1, 2, 2, 2, 2, 0, 5, 0, 7, 8, 8, 8, 0, -100]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf7d44f0-4cc1-4c58-8bb1-98203b615c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    1,    2,    2,    2,    2,    0,    5,    0,    7,    8,    8,\n",
       "            8,    0, -100]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_dataset[\"train\"][0]])\n",
    "batch[\"labels\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63888b6f-dddc-43fe-8d24-3c4e6b3c053e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    1,    2,    2,    2,    2,    0,    5,    0,    7,    8,    8,\n",
       "            8,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100],\n",
       "        [-100,    7,    8,    8,    0,    3,    4,    4,    4,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    7,    0,\n",
       "            0,    0, -100]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_dataset[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1464b4c5-620d-420b-a83c-dbf4e43e2246",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91862527-dd0f-44e9-a88a-9167726c910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_labels = train.features['ner_tags'].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da351dba-ce48-4ab3-870e-7256ed3aaf23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-PERSON',\n",
       " 'I-PERSON',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'B-LOCATION',\n",
       " 'I-LOCATION',\n",
       " 'B-MISC',\n",
       " 'I-MISC']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0412ed78-3963-40d7-98f9-05622c0cfbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 6.34kB [00:00, 14.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5ed102c-45ef-46de-9e0c-3c1d75e2b335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 0, 5, 0, 7, 0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataset[\"train\"][0][\"ner_tags\"]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d566bd26-48d8-4636-a109-726e363efaca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 532629\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e080a71a-c0f2-492d-9d36-d106a944aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset[\"train\"][0][\"ner_tags\"]\n",
    "labels = [ner_labels[i] for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a30b25ff-2401-434f-abc5-0f1a028e14c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-PERSON', 'I-PERSON', 'O', 'B-LOCATION', 'O', 'B-MISC', 'O']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b27ca52-ce54-46a5-893a-0deb58d70961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will check if the metric work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60006a03-b314-4318-a47d-7ed1a430d8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57a13be6-d29f-4d2a-9aa3-0fe0566a38c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example[1] = \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d985816b-1e27-4e54-a2df-b1aabb10ad7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-PERSON', 'I-PERSON', 'O', 'B-LOCATION', 'O', 'B-MISC', 'O']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9da9c428-49e5-4731-8458-767661a70ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-PERSON', 'O', 'O', 'B-LOCATION', 'O', 'B-MISC', 'O']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0d6fbb5-c79f-4a1e-9c4c-123e1fd88775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOCATION': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'MISC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'PERSON': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'overall_precision': 0.6666666666666666,\n",
       " 'overall_recall': 0.6666666666666666,\n",
       " 'overall_f1': 0.6666666666666666,\n",
       " 'overall_accuracy': 0.8571428571428571}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=[example],references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8c9b741-1d94-4ee3-b87e-43c3260fa430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[ner_labels[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [ner_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a0bd0d0-929f-4b23-ba5f-56a689ff9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(ner_labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3a42431-9910-4f2d-bc55-bbbfb9a10e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5969c6dc-5ac1-4406-8de1-50ffc835b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"output_ner\",\n",
    "    logging_strategy=\"steps\", #to make the training process fast\n",
    "    logging_steps=500, #to make the training process fast\n",
    "    per_device_train_batch_size=16, #to make it use more gpu \n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "82166325-24c9-4663-b5af-aa738c3c9d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "split_dataset = tokenized_dataset['train'].train_test_split(test_size = 0.1, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "60b26bf8-8a90-42eb-bb6a-ce9f8e4f41e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 532629\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29adf5ba-740d-4e82-95a9-b1ab5ac00212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 479366\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 53263\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2a1e8ea9-a6a6-4102-b644-3f148516774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DatasetDict({\n",
    "    'train': split_dataset['train'],\n",
    "    'validation': split_dataset['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cca0274c-a620-4846-9f34-3f662e603a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenClassifierOutput(loss=None, logits=tensor([[[-0.1449,  0.1564, -0.0080, -0.1890, -0.1102,  1.0029,  0.6703,\n",
      "          -0.7343,  0.4139],\n",
      "         [-0.3636, -0.9279,  0.3841,  0.4687, -0.0844,  0.3522, -0.1254,\n",
      "          -0.3245,  0.7734],\n",
      "         [-0.5048,  0.1473, -0.2862, -0.4492, -0.2279, -0.0546,  0.2735,\n",
      "           0.0658, -0.2605],\n",
      "         [-0.3211,  0.1372, -0.0402,  0.2147, -0.6757, -0.2528,  0.0072,\n",
      "          -0.6849,  0.2127],\n",
      "         [ 0.0730, -0.5232, -0.2662,  0.7784, -0.5404,  0.1430,  0.2655,\n",
      "          -0.2674,  0.7854],\n",
      "         [-0.5356, -0.2849, -0.3356,  0.1261, -0.8518,  0.4958,  0.0186,\n",
      "          -0.3732,  0.6601],\n",
      "         [-0.4989, -0.0532,  0.1087,  0.5134, -0.5403, -0.1699, -0.6500,\n",
      "           0.1798,  0.0928],\n",
      "         [-1.2003,  0.1072,  0.5625,  0.3486, -0.3021,  0.2907,  0.3294,\n",
      "          -0.3006,  0.3240],\n",
      "         [-0.4777, -0.0480,  0.0340, -0.1152, -0.3919,  0.5954, -0.6397,\n",
      "          -1.0572,  0.4845],\n",
      "         [-0.6239,  0.1282, -0.3619,  0.9212,  0.4104,  0.1441, -0.3460,\n",
      "          -0.9816,  0.5861],\n",
      "         [-0.5683,  0.1201, -0.6124, -0.1196,  0.0166, -0.0784, -0.7405,\n",
      "           0.9192,  0.8709]]], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer([\"Gokhan Ergul 8 Ülke gezdi\"], return_tensors=\"pt\", is_split_into_words=True)\n",
    "outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0c8a9b3f-70f6-4db4-90d9-8a033fecd22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 479366\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 53263\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12bb242f-9059-4bc9-a157-dc70f1409d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_simple = DatasetDict({\n",
    "    'train': ds['train'].select(range(5000)),\n",
    "    \"validation\": ds['validation'].select(range(2000))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "34ff2925-0a27-4ba6-aa1c-3d70ccb57f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1fe6afcc-816e-4065-a44a-371eaec28986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "485e330c-ebfb-4081-b6e9-88875c4361e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgokhannergull\u001b[0m (\u001b[33mgokhannergull-student\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gokhan/hugging_face/wandb/run-20251007_115647-ha3ufa80</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gokhannergull-student/huggingface/runs/ha3ufa80' target=\"_blank\">wobbly-oath-3</a></strong> to <a href='https://wandb.ai/gokhannergull-student/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gokhannergull-student/huggingface' target=\"_blank\">https://wandb.ai/gokhannergull-student/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gokhannergull-student/huggingface/runs/ha3ufa80' target=\"_blank\">https://wandb.ai/gokhannergull-student/huggingface/runs/ha3ufa80</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 00:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.493734</td>\n",
       "      <td>0.342811</td>\n",
       "      <td>0.348977</td>\n",
       "      <td>0.345866</td>\n",
       "      <td>0.808825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=313, training_loss=0.641147942588733, metrics={'train_runtime': 49.483, 'train_samples_per_second': 101.045, 'train_steps_per_second': 6.325, 'total_flos': 130595401352880.0, 'train_loss': 0.641147942588733, 'epoch': 1.0})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset = ds_simple['train'],\n",
    "    eval_dataset= ds_simple['validation'],\n",
    "    data_collator= data_collator,\n",
    "    compute_metrics= compute_metrics,\n",
    "    processing_class=tokenizer\n",
    "    \n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0cb17c69-144a-4905-87ea-9ce829a2961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_simple = DatasetDict({\n",
    "    'train': ds['train'].select(range(10000)),\n",
    "    \"validation\": ds['validation'].select(range(4000))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "01f55991-9bd0-40b5-be24-81f7c0801071",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 01:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.442464</td>\n",
       "      <td>0.417219</td>\n",
       "      <td>0.448346</td>\n",
       "      <td>0.432223</td>\n",
       "      <td>0.822802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=625, training_loss=0.4720489440917969, metrics={'train_runtime': 93.3317, 'train_samples_per_second': 107.145, 'train_steps_per_second': 6.697, 'total_flos': 261223466867424.0, 'train_loss': 0.4720489440917969, 'epoch': 1.0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset = ds_simple['train'],\n",
    "    eval_dataset= ds_simple['validation'],\n",
    "    data_collator= data_collator,\n",
    "    compute_metrics= compute_metrics,\n",
    "    processing_class=tokenizer\n",
    "    \n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "653f7d92-03e6-4b58-8a65-2e69b4c7dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a5994ec-0c75-499f-b4e8-ae94144f8164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1cb004dd-104a-47a1-9eaf-ede0627c1443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e62e475-0728-46ff-b795-b52a8de9c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "ner_pipe = pipeline('ner',model = model, tokenizer = tokenizer, aggregation_strategy = \"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "96369a8d-a197-4554-9366-8b0b263ef765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'MISC',\n",
       "  'score': 0.6844826,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 43,\n",
       "  'end': 55}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Merhaba ben Gökhan, İstanbul Kağıthane'den Hugging Face ailesine selamlar.\"\n",
    "results = ner_pipe(sentence)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2f659263-79f9-45ae-bf6b-1a391f54841e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29961' max='29961' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29961/29961 1:09:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.296900</td>\n",
       "      <td>0.294780</td>\n",
       "      <td>0.647345</td>\n",
       "      <td>0.669930</td>\n",
       "      <td>0.658444</td>\n",
       "      <td>0.883531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=29961, training_loss=0.334889830656807, metrics={'train_runtime': 4189.7839, 'train_samples_per_second': 114.413, 'train_steps_per_second': 7.151, 'total_flos': 1.258715000854818e+16, 'train_loss': 0.334889830656807, 'epoch': 1.0})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset = ds['train'],\n",
    "    eval_dataset= ds['validation'],\n",
    "    data_collator= data_collator,\n",
    "    compute_metrics= compute_metrics,\n",
    "    processing_class=tokenizer\n",
    "    \n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f86c5d3d-a47f-46e5-92e9-230ce4f4dc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "ner_pipe = pipeline('ner',model = model, tokenizer = tokenizer, aggregation_strategy = \"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "50d248cf-47a4-49b9-913b-a0383b24cb19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'MISC',\n",
       "  'score': 0.5826613,\n",
       "  'word': 'ben',\n",
       "  'start': 8,\n",
       "  'end': 11},\n",
       " {'entity_group': 'LOCATION',\n",
       "  'score': 0.6030029,\n",
       "  'word': \"İstanbul ' da\",\n",
       "  'start': 20,\n",
       "  'end': 31},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.39027178,\n",
       "  'word': '##gin',\n",
       "  'start': 46,\n",
       "  'end': 49}]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Merhaba ben Gökhan, İstanbul'da yaŞıyorumç Hugging Face ailesine selamlar.\"\n",
    "results = ner_pipe(sentence)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "826cfe2c-5ed8-43d0-9303-544fb01af3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-PERSON', 2: 'I-PERSON', 3: 'B-ORGANIZATION', 4: 'I-ORGANIZATION', 5: 'B-LOCATION', 6: 'I-LOCATION', 7: 'B-MISC', 8: 'I-MISC'}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ec45add5-ce24-45d5-a6cb-7c8087ffbd74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ahmet']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Ahmet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6a528a80-9ab2-48c5-adc9-26e637c2bc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_group': 'LOCATION', 'score': 0.5819567, 'word': \"İstanbul ' da\", 'start': 20, 'end': 31}\n",
      "{'entity_group': 'MISC', 'score': 0.41383007, 'word': 'Hugging', 'start': 43, 'end': 50}\n"
     ]
    }
   ],
   "source": [
    "ner_pipe = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=0\n",
    ")\n",
    "\n",
    "sentence = \"Merhaba ben Gökhan, İstanbul'da yaşıyorum. Hugging Face ailesine selamlar.\"\n",
    "results = ner_pipe(sentence)\n",
    "\n",
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f304c0d8-c8b3-474c-a6ee-1800d8b517be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ffc27ba1-8787-46b9-aba8-c21a56bbc3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Yönetim merkezi aynı isimli Kurgan şehri olan ilçe 2005 yılında Andhoy ilçesi'nden ayrılarak ilçe yapılmıştır.\n",
      "Example 2: Bunun ardından Başbakan Ali Mohammed Ghedi tüm ülkeye silahsızlanma çağrısında bulunmuştur.\n",
      "Example 3: Oxford Üniversitesi'nde siyaset doktorası yapmıştır.\n",
      "Example 4: Baysungur, tam adı Gıyaseddin Baysungur, Timurlu devlet adamı ve hattat.\n",
      "Example 5: 199192 NBA sezonu ABD profesyonel basketbol ligi NBA'in 46. sezonudur.\n"
     ]
    }
   ],
   "source": [
    "# Örnek sayısını belirle\n",
    "num_examples = 5\n",
    "\n",
    "# Validation datasetinden rastgele veya ilk birkaç örneği al\n",
    "examples = ds['validation'][:num_examples]  # ilk 5 örnek\n",
    "\n",
    "for i in range(num_examples):\n",
    "    input_ids = examples['input_ids'][i]\n",
    "    # Tokenleri kelimelere çevir\n",
    "    sentence = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    print(f\"Example {i+1}: {sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "69ebffd0-ff8b-4eab-abcd-dfbc17093c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Yönetim merkezi aynı isimli Kurgan şehri olan ilçe 2005 yılında Andhoy ilçesi'nden ayrılarak ilçe yapılmıştır.\n",
      "Example 2: Bunun ardından Başbakan Ali Mohammed Ghedi tüm ülkeye silahsızlanma çağrısında bulunmuştur.\n",
      "Example 3: Oxford Üniversitesi'nde siyaset doktorası yapmıştır.\n",
      "Example 4: Baysungur, tam adı Gıyaseddin Baysungur, Timurlu devlet adamı ve hattat.\n",
      "Example 5: 199192 NBA sezonu ABD profesyonel basketbol ligi NBA'in 46. sezonudur.\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_examples):\n",
    "    input_ids = examples['input_ids'][i]\n",
    "    # Tokenleri kelimelere çevir\n",
    "    sentence = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    print(f\"Example {i+1}: {sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3361bc9e-26b6-4e66-90d4-643021d77e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1: Yönetim merkezi aynı isimli Kurgan şehri olan ilçe 2005 yılında Andhoy ilçesi'nden ayrılarak ilçe yapılmıştır.\n",
      "{'entity_group': 'LOCATION', 'score': 0.6800206, 'word': 'Kurgan', 'start': 28, 'end': 34}\n",
      "{'entity_group': 'LOCATION', 'score': 0.90730876, 'word': 'Andhoy', 'start': 64, 'end': 70}\n",
      "{'entity_group': 'LOCATION', 'score': 0.58469677, 'word': 'nden', 'start': 78, 'end': 82}\n",
      "\n",
      "Sentence 2: Bunun ardından Başbakan Ali Mohammed Ghedi tüm ülkeye silahsızlanma çağrısında bulunmuştur.\n",
      "{'entity_group': 'MISC', 'score': 0.87300646, 'word': 'Başbakan', 'start': 15, 'end': 23}\n",
      "{'entity_group': 'PERSON', 'score': 0.44243264, 'word': 'Mohammed', 'start': 28, 'end': 36}\n",
      "{'entity_group': 'PERSON', 'score': 0.4504234, 'word': '##he', 'start': 38, 'end': 40}\n",
      "\n",
      "Sentence 3: Oxford Üniversitesi'nde siyaset doktorası yapmıştır.\n",
      "{'entity_group': 'ORGANIZATION', 'score': 0.8838109, 'word': \"Oxford Üniversitesi ' nde\", 'start': 0, 'end': 23}\n",
      "{'entity_group': 'MISC', 'score': 0.99340665, 'word': 'siyaset', 'start': 24, 'end': 31}\n",
      "\n",
      "Sentence 4: Baysungur, tam adı Gıyaseddin Baysungur, Timurlu devlet adamı ve hattat.\n",
      "{'entity_group': 'PERSON', 'score': 0.9344165, 'word': 'Baysungur', 'start': 0, 'end': 9}\n",
      "{'entity_group': 'PERSON', 'score': 0.7815961, 'word': 'Baysungur', 'start': 30, 'end': 39}\n",
      "\n",
      "Sentence 5: 199192 NBA sezonu ABD profesyonel basketbol ligi NBA'in 46. sezonudur.\n",
      "{'entity_group': 'MISC', 'score': 0.97677433, 'word': '199192', 'start': 0, 'end': 6}\n",
      "{'entity_group': 'MISC', 'score': 0.8172054, 'word': 'sezonu', 'start': 11, 'end': 17}\n",
      "{'entity_group': 'MISC', 'score': 0.98231804, 'word': 'basketbol', 'start': 34, 'end': 43}\n",
      "{'entity_group': 'MISC', 'score': 0.98562235, 'word': 'sezonudur', 'start': 60, 'end': 69}\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_examples):\n",
    "    input_ids = examples['input_ids'][i]\n",
    "    sentence = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    predictions = ner_pipe(sentence)\n",
    "    print(f\"\\nSentence {i+1}: {sentence}\")\n",
    "    for p in predictions:\n",
    "        print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ddf37fa8-1bc7-41ca-b9e0-5101525ab1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'output_ner/checkpoint-29961' klasöründeki dosyalar:\n",
      "- optimizer.pt\n",
      "- vocab.txt\n",
      "- model.safetensors\n",
      "- tokenizer.json\n",
      "- rng_state.pth\n",
      "- scaler.pt\n",
      "- trainer_state.json\n",
      "- tokenizer_config.json\n",
      "- scheduler.pt\n",
      "- training_args.bin\n",
      "- special_tokens_map.json\n",
      "- config.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"output_ner/checkpoint-29961\"\n",
    "\n",
    "files = os.listdir(output_dir)\n",
    "print(f\"'{output_dir}' klasöründeki dosyalar:\")\n",
    "for f in files:\n",
    "    print(\"-\", f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21ddda6-6116-433d-8182-e4888306123c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (GPU)",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

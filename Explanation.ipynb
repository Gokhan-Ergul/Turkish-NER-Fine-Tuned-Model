{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1d9a8b0-d154-4e54-931d-09405d3c1f8e",
   "metadata": {},
   "source": [
    "# Explain the difficult algorithm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149315f5-9351-4164-8da6-cf1cd1efd4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[ner_labels[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [ner_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39f40ec-410a-4785-966e-dbbd6fa3c027",
   "metadata": {},
   "source": [
    "O=0, PER=1, LOC=2, ORG=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be9128b0-7cd7-4c9b-a86a-6975dd4a2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Etiket isimleri\n",
    "ner_labels = [\"O\", \"PER\", \"LOC\", \"ORG\"]\n",
    "\n",
    "# 2 örnek cümle var, her biri 5 token uzunluğunda\n",
    "# logits: (batch_size=2, seq_len=5, num_labels=4)\n",
    "logits = np.array([\n",
    "    [  # 1. örnek\n",
    "        [2.1, 0.3, 0.2, 0.1],   # token 1 -> \"O\"\n",
    "        [0.1, 2.5, 0.2, 0.3],   # token 2 -> \"PER\"\n",
    "        [0.1, 0.2, 2.0, 0.4],   # token 3 -> \"LOC\"\n",
    "        [1.5, 0.1, 0.2, 0.1],   # token 4 -> \"O\"\n",
    "        [0.1, 0.1, 0.2, 0.3],   # token 5 -> \"ORG\"\n",
    "    ],\n",
    "    [  # 2. örnek\n",
    "        [0.2, 0.1, 2.3, 0.1],   # token 1 -> \"LOC\"\n",
    "        [0.1, 2.0, 0.2, 0.2],   # token 2 -> \"PER\"\n",
    "        [2.5, 0.1, 0.1, 0.2],   # token 3 -> \"O\"\n",
    "        [0.3, 0.1, 0.2, 2.0],   # token 4 -> \"ORG\"\n",
    "        [0.1, 0.1, 0.2, 0.3],   # token 5 -> \"ORG\"\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Gerçek etiketler (label id'leri)\n",
    "# -100 = özel token (örn. [CLS], [PAD]) => ignore edilir\n",
    "labels = np.array([\n",
    "    [0, 1, 2, 0, -100],  # 1. örnek\n",
    "    [2, 1, 0, 3, -100]   # 2. örnek\n",
    "])\n",
    "\n",
    "eval_preds = (logits, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9fea0b2-bcb6-4577-a3cc-0942d1df0fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6141bd81-12f3-4da7-8a4e-d3c98b6d7644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 0, 3],\n",
       "       [2, 1, 0, 3, 3]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80654046-b43a-4738-ad07-06bab0ade1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O', 'PER', 'LOC', 'O'], ['LOC', 'PER', 'O', 'ORG']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels_1 = [[ner_labels[l] for l in label if l != -100] for label in labels]\n",
    "true_labels_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7f2ec71-800e-42c0-a394-960f783a2624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2    0 -100]\n",
      "[   2    1    0    3 -100]\n"
     ]
    }
   ],
   "source": [
    "true_labels = []\n",
    "for label in labels:\n",
    "    print(label)\n",
    "    labels_list= []\n",
    "    for l in label:\n",
    "        if l != -100:\n",
    "            labels_list.append(ner_labels[l])\n",
    "\n",
    "    true_labels.append(labels_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddcd3d60-66ff-4b5d-8ed1-efce7f27c76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O', 'PER', 'LOC', 'O'], ['LOC', 'PER', 'O', 'ORG']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dbbe6e84-9862-4a9e-89de-18dcb54527ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O', 'PER', 'LOC', 'O'], ['LOC', 'PER', 'O', 'ORG']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_predictions = [\n",
    "    [ner_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a01a91d-fb20-4144-84d0-05d41e769723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O', 'PER', 'LOC', 'O'], ['LOC', 'PER', 'O', 'ORG']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_p = []\n",
    "for prediction, label in zip(predictions, labels):\n",
    "    true_p_list = []\n",
    "    for p,l in zip(prediction,label):\n",
    "        if l!= -100:\n",
    "            true_p_list.append(ner_labels[p])\n",
    "    true_p.append(true_p_list)\n",
    "\n",
    "true_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41019862-7ce2-45eb-8cd8-015e558978d9",
   "metadata": {},
   "source": [
    "# Explain the difficult algorithm 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64bc7dd7-7e9e-4a0b-9054-4fd2efd805eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalde Ankara güzeldir bizim veri setimizde şu şekilde yer alıyor: Ankara, Güzeldir, fakat model bunu şu şekilde tokenize ediyor: [CLS], \"An\", \"##kara\", \"güzel\", \"##dir\", [SEP]  gerçek etiketlerimiz de B-LOC,O olduğu için model hata verir tahmin yapmaz çünkü tokenized 6 oldu fakat etiket iki tane. Bu yüzden etiketleri tokenlere doğru bir şekilde yaymamız gerekiyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c32a0628-73c1-419c-b0b8-a540c232ddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = [None, 0, 0, 1, 1, None] #tokenized icin donen ids listesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe04d47-026c-47ee-9281-3b0479596b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids: # iclerine bakiyrouz , 0 a geldik,\n",
    "        if word_id != current_word: # ikisi de none o yuzden ilk adim olarak bunu atliyoruz. 0 none degil bu yuzden buraya girdik\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id] #labels'taki ilk etiket neyse onun strsini  aldik \n",
    "            new_labels.append(label)\n",
    "        elif word_id is None: # buraya geldik ve special token oldugu icin -100 koyduk\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else: # ikinci 0 icin buraya girdik.\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id] #yine index 0 olani aldik cunku ayni kelime fakat burda bu onceki kelimenin devami oldugu icin labels'taki etiket baslangic degil devam niteliginde bu yuzden onu I-...  yapmamiz lazim (tek sayi ise onu cift yap)\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label) #sonra yine str olarak al donguyu boylece devam ettir.\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e6630e-1e6f-48ec-8c26-a29c54072d4a",
   "metadata": {},
   "source": [
    "sonuc :\n",
    "\n",
    "[CLS]   An   ##kara  güzel  ##dir  [SEP]\n",
    "\n",
    "\n",
    " -100   1     2       0       0    -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10cce2be-2e9c-4767-a3df-1a3cfea37b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_align_labels(example):\n",
    "    tokenize  = tokenizer(example['tokens'], is_split_into_words=True)\n",
    "    all_labels = example['ner_tags']\n",
    "    new_labels = []\n",
    "    deleted_row_count = 0\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_inds = tokenize.word_ids(i)\n",
    "        \n",
    "        valid_word_inds = [w for w in word_inds if w is not None]\n",
    "        if len(valid_word_inds) > 0 and max(valid_word_inds) >= len(labels): #some rows has labels and words not equal.\n",
    "            print(\"WARNING: word_inds out of range!\", word_inds, \"labels len:\", len(labels))\n",
    "            new_labels.append([-100]*len(word_ids))\n",
    "            deleted_row_count+=1\n",
    "            continue\n",
    "            \n",
    "        new_labels.append(align_labels_with_tokens(labels,word_inds))\n",
    "\n",
    "\n",
    "    tokenize['labels'] = new_labels\n",
    "    return tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b350c271-7712-420a-8995-c745a2299daa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (GPU)",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
